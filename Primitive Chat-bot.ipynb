{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #                                   Simple & Primitive Chatbot   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dhakshesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dhakshesh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import numpy as np\n",
    "import random \n",
    "import string\n",
    "f=open('C:/Users/Dhakshesh/Documents/Spyder/chatbot.txt','r',errors='ignore')\n",
    "raw=f.read()\n",
    "raw=raw.lower()\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "sent_tokens=nltk.sent_tokenize(raw)\n",
    "word_tokens=nltk.word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example I have used certain data from the wikipedia page about chatbots and certain information about the types of corpses. You can use a corpus of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hotpotqa is a set of question response data that includes natural multi-skip questions, with a strong emphasis on supporting facts to allow for more explicit question answering systems.',\n",
       " 'the data set consists of 113,000 wikipedia-based qa pairs.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the text  \n",
    "\n",
    "The functions declared below \"LemTokens\" will help us take input as the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer =  nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return[lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword matching\n",
    "Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response. ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_inputs = (\"hello\" , \"hi\" ,\"greetings\", \"sup\", \"what's up\",\"hey\")\n",
    "greeting_responses= [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greeting_inputs:\n",
    "            return random.choice(greeting_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Response\n",
    "To generate a response from our bot for input questions, the concept of document similarity will be used. So we begin by importing necessary modules.\n",
    "* From scikit learn library, import the TFidf vectorizer to convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "* Also, import cosine similarity module from scikit learn library.This will be used to find the similarity between words entered by the user and the words in the corpus. \n",
    "* This is the simplest possible implementation of a chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    \n",
    "    TfidfVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1],tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I dont understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response + sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatto: My name is Chatto. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "Hello\n",
      "Chatto: *nods*\n",
      "Hi\n",
      "Chatto: hi\n",
      "What is HotpotQA?\n",
      "Chatto: hotpotqa is a set of question response data that includes natural multi-skip questions, with a strong emphasis on supporting facts to allow for more explicit question answering systems.\n",
      "Describe chatbot.\n",
      "Chatto: a chatbot is a software application used to conduct an on-line chat conversation via text or text-to-speech, in lieu of providing direct contact with a live human agent.\n",
      "What is TyDi?\n",
      "Chatto: tydi qa is a set of question response data covering 11 typologically diverse languages with 204k question-answer pairs.\n",
      "Bye.\n",
      "Chatto : Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"Chatto: My name is Chatto. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you'):\n",
    "            print(\"Chatto:You are welcome\")\n",
    "        elif ( 'bye' in user_response ):\n",
    "            flag=False\n",
    "            print(\"Chatto : Bye! take care..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"Chatto: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"Chatto: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"Chatto : Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats! \n",
    "You have built a primitive, non generative chatbot all by yourself. Please do not limit yourself go on and keep building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
